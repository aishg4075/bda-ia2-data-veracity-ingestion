{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a273bd5f",
   "metadata": {},
   "source": [
    "# 1. Title + Aim\n",
    "\n",
    "## Data Veracity and Integrity in Distributed Systems: Ensuring Trust in Information Ingestion\n",
    "\n",
    "**Topic 17 case study:** veracity validation for high-velocity web advertising clickstream events with attribution-surrogate modeling.\n",
    "\n",
    "### Aim\n",
    "This notebook implements an end-to-end, reproducible pipeline that:\n",
    "\n",
    "1. Validates data veracity at ingestion using explicit data quality checks.\n",
    "2. Enforces integrity with tamper-evident audit logging (hashing + chain verification).\n",
    "3. Compares Spark batch processing and streaming ingestion (Kafka-first, fallback file stream).\n",
    "4. Trains and evaluates attribution-surrogate classifiers under class imbalance using robust metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c88fd",
   "metadata": {},
   "source": [
    "# 2. Problem Statement & Why Veracity/Integrity Matter (1–2 paragraphs)\n",
    "\n",
    "Ad-tech systems ingest massive, high-velocity event streams where fraudulent or low-quality records can quickly degrade model quality, budget efficiency, and operational trust. In this environment, accuracy alone is not enough: organizations must verify that each ingested record is structurally valid, complete, plausible, and non-duplicative. Without ingestion-time veracity checks, downstream analytics and ML systems can optimize on corrupted signals.\n",
    "\n",
    "Integrity is the second trust pillar. Even if records are validated at ingestion, stakeholders still need confidence that logs and batch outputs were not altered post hoc. For this reason, this notebook implements a tamper-evident audit trail using per-record cryptographic hashing, Merkle roots, and a chained batch hash. Together, veracity + integrity provide practical trust guarantees for distributed information ingestion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edc148c",
   "metadata": {},
   "source": [
    "# 3. Dataset Source + Description + Schema\n",
    "\n",
    "For final validated reporting in this repository, the run uses Kaggle TalkingData `data/train.csv` with a deterministic cap of 300,000 rows.\n",
    "\n",
    "Supervised target for this implementation is Kaggle `is_attributed` (attribution/conversion proxy), used as a surrogate target for demonstrating veracity/integrity-aware ingestion and classification. It is not a native fraud ground-truth label.\n",
    "\n",
    "### Event schema used by the pipeline\n",
    "\n",
    "- `click_time` (event timestamp)\n",
    "- `ip` (user surrogate)\n",
    "- `app`, `device`, `os`, `channel` (context/device/app identifiers)\n",
    "- `is_attributed` (supervised target for this implementation)\n",
    "- optional `attributed_time`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Resolve repository root robustly from notebook or repo execution context.\n",
    "CWD = Path.cwd().resolve()\n",
    "if (CWD / \"src\").exists():\n",
    "    PROJECT_ROOT = CWD\n",
    "elif (CWD.parent / \"src\").exists():\n",
    "    PROJECT_ROOT = CWD.parent\n",
    "else:\n",
    "    raise RuntimeError(\"Could not resolve PROJECT_ROOT containing src/\")\n",
    "\n",
    "SRC_PATH = PROJECT_ROOT / \"src\"\n",
    "if str(SRC_PATH) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_PATH))\n",
    "\n",
    "from ingestion_trust.core import (\n",
    "    append_audit_batch,\n",
    "    build_run_metadata,\n",
    "    measure_integrity_overhead,\n",
    "    plot_data_quality_summary,\n",
    "    plot_fraud_prevalence,\n",
    "    save_json,\n",
    "    tamper_log_copy,\n",
    "    train_and_evaluate_models,\n",
    "    verify_audit_log,\n",
    ")\n",
    "from ingestion_trust.spark_utils import (\n",
    "    apply_schema_casting,\n",
    "    build_baseline_stats,\n",
    "    compute_veracity_score,\n",
    "    create_spark_session,\n",
    "    feature_engineering,\n",
    "    get_event_schema,\n",
    "    run_veracity_checks,\n",
    ")\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "FIGURES_DIR = PROJECT_ROOT / \"reports\" / \"figures\"\n",
    "METRICS_DIR = ARTIFACTS_DIR / \"metrics\"\n",
    "MODELS_DIR = ARTIFACTS_DIR / \"models\"\n",
    "AUDIT_DIR = ARTIFACTS_DIR / \"audit\"\n",
    "PROVENANCE_DIR = ARTIFACTS_DIR / \"provenance\"\n",
    "CURATED_DIR = ARTIFACTS_DIR / \"curated\"\n",
    "\n",
    "for p in [METRICS_DIR, MODELS_DIR, AUDIT_DIR, PROVENANCE_DIR, CURATED_DIR, FIGURES_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASET_PATH = DATA_DIR / \"sample_click_fraud.csv\"\n",
    "if not DATASET_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Sample dataset missing at {DATASET_PATH}\")\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATASET_PATH:\", DATASET_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f0ab4",
   "metadata": {},
   "source": [
    "# 4. Environment & Experimental Setup (print versions + hardware summary)\n",
    "\n",
    "This section records runtime details for reproducibility and provenance:\n",
    "\n",
    "- Python and package versions\n",
    "- OS and hardware summary\n",
    "- Spark session configuration (local mode)\n",
    "- Deterministic seed settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba3dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def _pkg_version(name: str):\n",
    "    try:\n",
    "        mod = __import__(name)\n",
    "        return getattr(mod, \"__version__\", \"unknown\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "env_summary = {\n",
    "    \"python_version\": sys.version,\n",
    "    \"platform\": platform.platform(),\n",
    "    \"machine\": platform.machine(),\n",
    "    \"processor\": platform.processor(),\n",
    "    \"numpy\": _pkg_version(\"numpy\"),\n",
    "    \"pandas\": _pkg_version(\"pandas\"),\n",
    "    \"sklearn\": _pkg_version(\"sklearn\"),\n",
    "    \"xgboost\": _pkg_version(\"xgboost\"),\n",
    "    \"pyspark\": _pkg_version(\"pyspark\"),\n",
    "}\n",
    "\n",
    "print(json.dumps(env_summary, indent=2))\n",
    "\n",
    "spark = create_spark_session(\"topic17-notebook-batch\", master=\"local[*]\")\n",
    "print(\"Spark version:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81555d0",
   "metadata": {},
   "source": [
    "# 5. Batch Pipeline (Spark): ETL + Validation + Feature Engineering\n",
    "\n",
    "In this section we run the Spark batch ETL path:\n",
    "\n",
    "1. Read CSV with explicit schema.\n",
    "2. Apply deterministic type casting.\n",
    "3. Perform feature engineering used by both batch and streaming.\n",
    "4. Persist curated output as parquet after validation (next section).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12585803",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_etl = time.perf_counter()\n",
    "\n",
    "raw_df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .schema(get_event_schema())\n",
    "    .csv(str(DATASET_PATH))\n",
    ")\n",
    "\n",
    "print(\"Raw rows:\", raw_df.count())\n",
    "raw_df.show(5, truncate=False)\n",
    "\n",
    "cast_df = apply_schema_casting(raw_df)\n",
    "feature_df = feature_engineering(cast_df)\n",
    "\n",
    "print(\"Feature columns:\", len(feature_df.columns))\n",
    "feature_df.select(\n",
    "    \"click_time\", \"click_time_ts\", \"ip\", \"app\", \"device\", \"os\", \"channel\", \"is_attributed\", \"clicks_per_ip_per_hour\"\n",
    ").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79acf65",
   "metadata": {},
   "source": [
    "# 6. Veracity Checks (rules, scoring, summary tables, plots)\n",
    "\n",
    "The veracity layer applies six rule families:\n",
    "\n",
    "- Schema validity\n",
    "- Completeness\n",
    "- Range checks\n",
    "- Uniqueness (duplicate detection)\n",
    "- Drift checks versus baseline profile\n",
    "- Outlier checks on `clicks_per_ip_per_hour`\n",
    "\n",
    "A weighted per-record **veracity_score** (0 to 1) is then computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd458f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_stats = build_baseline_stats(feature_df)\n",
    "save_json(baseline_stats, METRICS_DIR / \"baseline_stats.json\")\n",
    "\n",
    "validated_df, validation_summary = run_veracity_checks(feature_df, baseline_stats=baseline_stats)\n",
    "scored_df = compute_veracity_score(validated_df)\n",
    "\n",
    "save_json(validation_summary, METRICS_DIR / \"validation_report.json\")\n",
    "\n",
    "summary_rows = []\n",
    "for rule, invalid_count in validation_summary[\"invalid_counts_by_rule\"].items():\n",
    "    summary_rows.append(\n",
    "        {\n",
    "            \"rule\": rule,\n",
    "            \"invalid_count\": invalid_count,\n",
    "            \"invalid_percent\": validation_summary[\"invalid_percent_by_rule\"][rule],\n",
    "        }\n",
    "    )\n",
    "validation_df = pd.DataFrame(summary_rows)\n",
    "validation_df.to_csv(METRICS_DIR / \"validation_report.csv\", index=False)\n",
    "\n",
    "plot_data_quality_summary(validation_df, FIGURES_DIR / \"data_quality_violations.png\")\n",
    "\n",
    "# Keep valid records for downstream model training.\n",
    "curated_df = scored_df.filter(\"is_valid_record = true\")\n",
    "curated_path = CURATED_DIR / \"batch_curated.parquet\"\n",
    "curated_df.write.mode(\"overwrite\").parquet(str(curated_path))\n",
    "\n",
    "etl_time_sec = time.perf_counter() - start_etl\n",
    "\n",
    "print(\"Validation summary:\")\n",
    "print(json.dumps(validation_summary, indent=2))\n",
    "print(\"ETL + validation time (sec):\", round(etl_time_sec, 3))\n",
    "print(\"Curated rows:\", curated_df.count())\n",
    "print(\"Curated parquet:\", curated_path)\n",
    "\n",
    "display(validation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec8f3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prevalence figure from curated dataset.\n",
    "curated_pd_preview = curated_df.select(\"is_attributed\").toPandas()\n",
    "plot_fraud_prevalence(curated_pd_preview, FIGURES_DIR / \"attribution_prevalence.png\")\n",
    "\n",
    "# Build and save provenance metadata.\n",
    "column_schema = [{\"name\": name, \"type\": dtype} for name, dtype in curated_df.dtypes]\n",
    "run_metadata = build_run_metadata(\n",
    "    dataset_path=DATASET_PATH,\n",
    "    dataset_source=\"sample_click_fraud.csv (local sample)\",\n",
    "    dataset_mode=\"sample\",\n",
    "    column_schema=column_schema,\n",
    "    repo_dir=PROJECT_ROOT,\n",
    "    extra={\n",
    "        \"etl_time_sec\": etl_time_sec,\n",
    "        \"curated_output_parquet\": str(curated_path),\n",
    "        \"validation_report\": str(METRICS_DIR / \"validation_report.json\"),\n",
    "    },\n",
    ")\n",
    "save_json(run_metadata, PROVENANCE_DIR / \"run_metadata.json\")\n",
    "print(json.dumps(run_metadata, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408357d2",
   "metadata": {},
   "source": [
    "# 7. Model Training & Evaluation (all required metrics + plots)\n",
    "\n",
    "Models trained in this section:\n",
    "\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- XGBoost (if available)\n",
    "\n",
    "Imbalance handling is applied via class weighting / `scale_pos_weight`.\n",
    "\n",
    "Metrics computed from actual run outputs:\n",
    "\n",
    "- Accuracy, Precision, Recall, F1\n",
    "- ROC-AUC, PR-AUC\n",
    "- MCC\n",
    "- Execution times\n",
    "\n",
    "A sanity check for the all-negative classifier is also included to show why raw accuracy can be misleading under class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fceaff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train = time.perf_counter()\n",
    "curated_pd = curated_df.toPandas()\n",
    "\n",
    "model_summary = train_and_evaluate_models(\n",
    "    curated_df=curated_pd,\n",
    "    artifacts_metrics_dir=METRICS_DIR,\n",
    "    figures_dir=FIGURES_DIR,\n",
    "    model_dir=MODELS_DIR,\n",
    "    seed=SEED,\n",
    ")\n",
    "training_time_sec = time.perf_counter() - start_train\n",
    "\n",
    "metrics_df = pd.read_csv(METRICS_DIR / \"model_metrics.csv\")\n",
    "\n",
    "display(metrics_df)\n",
    "print(\"Model run summary:\")\n",
    "print(json.dumps(model_summary, indent=2))\n",
    "print(\"Model training block time (sec):\", round(training_time_sec, 3))\n",
    "\n",
    "# Save split prevalence figure for report.\n",
    "split_prev = pd.DataFrame(\n",
    "    [\n",
    "        {\"split\": k, \"attributed_prevalence\": v}\n",
    "        for k, v in model_summary.get(\"class_prevalence\", {}).items()\n",
    "    ]\n",
    ")\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(data=split_prev, x=\"split\", y=\"attributed_prevalence\", palette=\"crest\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Attributed Prevalence by Split\")\n",
    "plt.xlabel(\"Split\")\n",
    "plt.ylabel(\"Attributed Class Rate\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"attribution_split_distribution.png\", dpi=150)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f4a481",
   "metadata": {},
   "source": [
    "# 8. Streaming Pipeline (Kafka + Spark Structured Streaming)\n",
    "\n",
    "The streaming implementation is provided in `scripts/run_stream_pipeline.py` and reuses the same shared functions as batch:\n",
    "\n",
    "- `apply_schema_casting`\n",
    "- `feature_engineering`\n",
    "- `run_veracity_checks`\n",
    "- `compute_veracity_score`\n",
    "\n",
    "## Default path: Kafka\n",
    "\n",
    "1. Start Kafka via Docker (`docker/docker-compose.kafka.yml`).\n",
    "2. Publish events with `scripts/kafka_producer.py`.\n",
    "3. Run the stream pipeline with Kafka source.\n",
    "\n",
    "## Fallback path: File stream\n",
    "\n",
    "If Kafka is not available, run Structured Streaming from file source while keeping the same validation/inference logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6082e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex\n",
    "import subprocess\n",
    "\n",
    "RUN_STREAMING_DEMO = False  # Set True to execute fallback streaming demo in-notebook.\n",
    "\n",
    "kafka_instructions = [\n",
    "    \"cd docker && docker compose -f docker-compose.kafka.yml up -d\",\n",
    "    \"python scripts/kafka_producer.py --input-csv data/sample_click_fraud.csv --topic clickstream_events\",\n",
    "    \"python scripts/run_stream_pipeline.py --kafka-bootstrap localhost:9092 --topic clickstream_events --model-path artifacts/models/best_model_pipeline.joblib\",\n",
    "]\n",
    "\n",
    "print(\"Kafka streaming commands:\")\n",
    "for cmd in kafka_instructions:\n",
    "    print(\" -\", cmd)\n",
    "\n",
    "if RUN_STREAMING_DEMO:\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        str(PROJECT_ROOT / \"scripts\" / \"run_stream_pipeline.py\"),\n",
    "        \"--fallback-file-source\",\n",
    "        str(PROJECT_ROOT / \"data\"),\n",
    "        \"--max-batches\",\n",
    "        \"2\",\n",
    "        \"--trigger-seconds\",\n",
    "        \"4\",\n",
    "        \"--model-path\",\n",
    "        str(PROJECT_ROOT / \"artifacts\" / \"models\" / \"best_model_pipeline.joblib\"),\n",
    "    ]\n",
    "    print(\"Running fallback stream demo:\", \" \".join(shlex.quote(c) for c in cmd))\n",
    "    proc = subprocess.run(cmd, cwd=str(PROJECT_ROOT), text=True, capture_output=True)\n",
    "    print(proc.stdout)\n",
    "    if proc.returncode != 0:\n",
    "        print(proc.stderr)\n",
    "        raise RuntimeError(\"Streaming demo failed\")\n",
    "else:\n",
    "    print(\"RUN_STREAMING_DEMO is False; streaming implementation is available via scripts and optional notebook execution.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534bbf4",
   "metadata": {},
   "source": [
    "### Comparative Discussion: Batch (Spark) vs Streaming (Kafka + Spark) vs Storm\n",
    "\n",
    "| Dimension | Spark Batch | Kafka + Spark Structured Streaming | Apache Storm (conceptual comparison) |\n",
    "|---|---|---|---|\n",
    "| Processing model | Finite datasets, scheduled ETL/training jobs | Continuous micro-batches with unified DataFrame API | Record-by-record tuple processing topology |\n",
    "| Latency profile | Higher latency, high throughput for large jobs | Lower latency than batch; tunable trigger intervals | Potentially very low latency with custom bolt design |\n",
    "| Developer model | Strong SQL/DataFrame support, easier reproducibility | Reuses same batch logic and schemas in many cases | More custom topology logic; higher operational complexity |\n",
    "| Fault tolerance | Checkpoint/restart by job | Exactly-once-like behavior with checkpoint + Kafka offsets | Acknowledgement trees and replay mechanisms |\n",
    "| Fit for this use case | Baseline quality profiling and offline model training | Near-real-time ingestion validation + scoring | Useful when ultra-low latency and custom operators dominate |\n",
    "\n",
    "In this project, Kafka + Spark Structured Streaming is implemented as the default streaming path because it maximizes code reuse with batch Spark validation and reduces implementation divergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c00841b",
   "metadata": {},
   "source": [
    "# 9. Integrity & Tamper-Evident Audit Logging (hash chain/Merkle + verification + tamper demo)\n",
    "\n",
    "Each batch writes an append-only JSONL audit entry containing:\n",
    "\n",
    "- `record_hashes` (SHA-256 of canonicalized records)\n",
    "- `merkle_root`\n",
    "- `prev_batch_hash`\n",
    "- `batch_hash = sha256(prev_batch_hash + merkle_root + metadata + batch_id + count)`\n",
    "\n",
    "We demonstrate both normal verification and tamper detection below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a3d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_log_path = AUDIT_DIR / \"audit_log.jsonl\"\n",
    "if audit_log_path.exists():\n",
    "    audit_log_path.unlink()\n",
    "\n",
    "records_for_audit = curated_pd.head(6000).to_dict(orient=\"records\")\n",
    "chunks = [records_for_audit[i:i+2000] for i in range(0, len(records_for_audit), 2000)]\n",
    "\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    append_audit_batch(\n",
    "        log_path=audit_log_path,\n",
    "        batch_id=f\"nb_batch_{idx}\",\n",
    "        records=chunk,\n",
    "        batch_metadata={\n",
    "            \"stage\": \"notebook_demo\",\n",
    "            \"chunk_index\": idx,\n",
    "            \"records\": len(chunk),\n",
    "        },\n",
    "    )\n",
    "\n",
    "verification_ok = verify_audit_log(audit_log_path)\n",
    "save_json(verification_ok, AUDIT_DIR / \"verification_report.json\")\n",
    "\n",
    "# Tamper demonstration\n",
    "Tampered = tamper_log_copy(audit_log_path, AUDIT_DIR / \"audit_log_tampered_demo.jsonl\")\n",
    "verification_bad = verify_audit_log(Tampered)\n",
    "save_json(verification_bad, AUDIT_DIR / \"verification_report_tampered.json\")\n",
    "\n",
    "print(\"Normal verification:\")\n",
    "print(json.dumps(verification_ok, indent=2))\n",
    "print(\"\n",
    "Tampered verification:\")\n",
    "print(json.dumps(verification_bad, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc337329",
   "metadata": {},
   "source": [
    "# 10. Integrity Overhead Measurement (throughput/latency with/without hash chain)\n",
    "\n",
    "To quantify integrity overhead, we compare:\n",
    "\n",
    "- baseline processing without hashing,\n",
    "- processing with record hashing + Merkle root generation.\n",
    "\n",
    "This gives a practical throughput and latency tradeoff estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f6567",
   "metadata": {},
   "outputs": [],
   "source": [
    "overhead_stats = measure_integrity_overhead(records_for_audit[:5000], loops=5)\n",
    "save_json(overhead_stats, METRICS_DIR / \"integrity_overhead.json\")\n",
    "\n",
    "overhead_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"mode\": \"without_integrity\",\n",
    "            \"throughput_rps\": overhead_stats[\"throughput_plain_rps\"],\n",
    "            \"latency_sec\": overhead_stats[\"avg_plain_sec\"],\n",
    "        },\n",
    "        {\n",
    "            \"mode\": \"with_integrity\",\n",
    "            \"throughput_rps\": overhead_stats[\"throughput_hash_rps\"],\n",
    "            \"latency_sec\": overhead_stats[\"avg_hash_sec\"],\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(overhead_df)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n",
    "sns.barplot(data=overhead_df, x=\"mode\", y=\"throughput_rps\", ax=axes[0], palette=\"mako\")\n",
    "axes[0].set_title(\"Throughput Comparison\")\n",
    "axes[0].set_xlabel(\"Mode\")\n",
    "axes[0].set_ylabel(\"Records / second\")\n",
    "axes[0].tick_params(axis=\"x\", rotation=20)\n",
    "\n",
    "sns.barplot(data=overhead_df, x=\"mode\", y=\"latency_sec\", ax=axes[1], palette=\"rocket\")\n",
    "axes[1].set_title(\"Latency Comparison\")\n",
    "axes[1].set_xlabel(\"Mode\")\n",
    "axes[1].set_ylabel(\"Seconds\")\n",
    "axes[1].tick_params(axis=\"x\", rotation=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"audit_overhead_throughput_latency.png\", dpi=150)\n",
    "plt.close(fig)\n",
    "\n",
    "print(json.dumps(overhead_stats, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffd8ae5",
   "metadata": {},
   "source": [
    "# 11. Scalability Note (what scales, what doesn’t; optionally replay larger chunks)\n",
    "\n",
    "## What scales well\n",
    "\n",
    "- Spark ETL and validation rules are DataFrame-based and distribute across partitions.\n",
    "- Structured Streaming applies the same transformations per micro-batch.\n",
    "- Audit logging can be attached per micro-batch with bounded sampling for record-hash payload size.\n",
    "\n",
    "## What needs care at larger scale\n",
    "\n",
    "- `toPandas()` conversions are practical for coursework-scale runs but should be replaced with Spark-native model serving or vectorized UDF patterns for very large streams.\n",
    "- Full `record_hashes` storage in JSONL may grow quickly; in production, store hash manifests/object storage references.\n",
    "- Drift calculations based on full cardinality should use bounded sketches or sampled histograms for high-cardinality columns.\n",
    "\n",
    "## Optional scale experiment\n",
    "\n",
    "Replay larger chunks by increasing `--max-events` in the Kafka producer and comparing:\n",
    "\n",
    "- micro-batch latency (`artifacts/metrics/streaming_metrics.csv`)\n",
    "- integrity overhead (`artifacts/metrics/integrity_overhead.json`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec345a",
   "metadata": {},
   "source": [
    "# 12. Social Impact Analysis (attribution-target monitoring + broader ingestion trust)\n",
    "\n",
    "Reliable detection of invalid traffic improves fairness and efficiency for advertisers, publishers, and users by reducing budget waste and suppressing abusive behavior. Veracity-aware ingestion also reduces the chance that downstream models amplify noisy or manipulated data.\n",
    "\n",
    "Beyond ad-tech, the same design pattern applies to healthcare, finance, and public-sector analytics where ingestion trust is critical. Explicit validation rules, provenance metadata, and tamper-evident logs improve accountability and auditability, which are key for responsible data-driven decision systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070679cb",
   "metadata": {},
   "source": [
    "# 13. Research Alignment (map implemented components to the research papers list)\n",
    "\n",
    "This repository keeps reference extraction source-grounded:\n",
    "\n",
    "- `docs/references.md` is generated only from provided XLSX/PDF sources.\n",
    "- `docs/research_alignment.md` maps implemented modules to literature themes and is updated with numbered citations after extraction.\n",
    "\n",
    "Current status in this environment:\n",
    "\n",
    "- Reference extraction source files may need to be copied into the repository root before regeneration.\n",
    "- Run `python scripts/generate_references.py --xlsx \"Reference List BDA IA2.xlsx\" --pdf \"BDA_GRP12_IA2_LABCA (1).pdf\"` to refresh citations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1656260c",
   "metadata": {},
   "source": [
    "### Diagnostics Summary\n",
    "- Leakage checks: no forbidden target/post-event features (`is_attributed`, legacy `is_fraud`, `attributed_time`) were present in the training matrix.\n",
    "- Split-overlap checks: event-key overlap across train/val/test was zero.\n",
    "- Metric parity checks: independently recomputed metrics matched `artifacts/metrics/model_metrics.csv`.\n",
    "- Audit overhead benchmark uses repeated runs (warmup + loops) and reports median/dispersion.\n",
    "- Best-model selection criterion documented: PR-AUC -> MCC -> F1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8bbb75",
   "metadata": {},
   "source": [
    "# 14. Conclusion + Next Steps\n",
    "\n",
    "This notebook implemented a complete ingestion-trust pipeline for ad-click event streams, combining veracity checks, tamper-evident integrity controls, and batch-vs-stream processing pathways in a single reproducible workflow.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "1. Replace fallback/sample data with full production-scale stream replay.\n",
    "2. Move model inference fully into Spark-native serving for large streams.\n",
    "3. Add online drift alarms and adaptive rule thresholds.\n",
    "4. Extend audit-chain verification into external immutable storage or ledger-backed systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b641e6",
   "metadata": {},
   "source": [
    "### Results Summary\n",
    "- All metrics shown below are generated from this run and saved under `artifacts/metrics`.\n",
    "- Final summary values: `artifacts/metrics/final_results_summary.json` and `artifacts/metrics/final_results_summary.csv`.\n",
    "- Model table: `artifacts/metrics/model_metrics.csv`.\n",
    "- Threshold analysis table: `artifacts/metrics/threshold_analysis_best_model.csv`.\n",
    "- Validation summary: `artifacts/metrics/validation_report.csv` and `artifacts/metrics/validation_report.json`.\n",
    "- Streaming metrics: `artifacts/metrics/stream_metrics.csv` and `artifacts/metrics/stream_pipeline_summary.json`.\n",
    "- Figures used in report/presentation: `reports/figures/`.\n",
    "- Supervised target caveat: Kaggle `is_attributed` is used as a surrogate attribution/conversion target, not native fraud ground-truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final run summary artifact for quick grading/reference.\n",
    "run_summary = {\n",
    "    \"dataset\": str(DATASET_PATH),\n",
    "    \"etl_time_sec\": etl_time_sec,\n",
    "    \"training_time_sec\": training_time_sec,\n",
    "    \"curated_parquet\": str(CURATED_DIR / \"batch_curated.parquet\"),\n",
    "    \"validation_report\": str(METRICS_DIR / \"validation_report.json\"),\n",
    "    \"model_metrics\": str(METRICS_DIR / \"model_metrics.csv\"),\n",
    "    \"audit_verification\": verification_ok,\n",
    "    \"tamper_verification\": verification_bad,\n",
    "}\n",
    "save_json(run_summary, METRICS_DIR / \"notebook_run_summary.json\")\n",
    "print(json.dumps(run_summary, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
